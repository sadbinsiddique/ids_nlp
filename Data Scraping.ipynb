{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Web Scraping using R**\n",
        "---\n",
        "**âœ… Scrap URL:** [Rising BD](https://www.risingbd.com/english)\n",
        "\n",
        "**âœ… Install Required Libraries**\n",
        "\n",
        "**âœ… Load Required Libraries**\n",
        "\n",
        "**âœ… Define User-Agent**\n",
        "\n",
        "**âœ… Function to Clean and Format Published Date**\n",
        "\n",
        "**âœ… Function to Web Scraping Logic**\n",
        "*   Extract elements\n",
        "*   Check for valid data\n",
        "*   Random delay\n",
        "\n",
        "**âœ… Load Link & Max Page**\n",
        "\n",
        "**âœ… Start Web Scraping**\n",
        "\n",
        "**âœ… Save Results**\n",
        "\n",
        "**âœ… Convert csv & save**"
      ],
      "metadata": {
        "id": "xihjNeRH4I3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries\n",
        "#install.packages(c(\"rvest\", \"httr\", \"stringr\", \"dplyr\", \"lubridate\"))"
      ],
      "metadata": {
        "id": "RY9mvV0T68b-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Required Libraries\n",
        "library(rvest)\n",
        "library(httr)\n",
        "library(stringr)\n",
        "library(dplyr)\n",
        "library(lubridate)"
      ],
      "metadata": {
        "id": "4dEuDOX_4Mwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2675542-e456-4eb2-d124-24d581f99f83"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Attaching package: â€˜dplyrâ€™\n",
            "\n",
            "\n",
            "The following objects are masked from â€˜package:statsâ€™:\n",
            "\n",
            "    filter, lag\n",
            "\n",
            "\n",
            "The following objects are masked from â€˜package:baseâ€™:\n",
            "\n",
            "    intersect, setdiff, setequal, union\n",
            "\n",
            "\n",
            "\n",
            "Attaching package: â€˜lubridateâ€™\n",
            "\n",
            "\n",
            "The following objects are masked from â€˜package:baseâ€™:\n",
            "\n",
            "    date, intersect, setdiff, union\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€ Define User-Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ua <- httr::user_agent(\"Chrome/134.0.0.0\")"
      ],
      "metadata": {
        "id": "_3zmwdsQ6t79"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Clean and Format Published Date\n",
        "clean_published_date <- function(raw_text) {\n",
        "  if (is.na(raw_text) || raw_text == \"\") return(NA_character_)\n",
        "\n",
        "  match <- str_extract(raw_text, \"^[^U]+\")\n",
        "  if (is.na(match)) return(NA_character_)\n",
        "\n",
        "  match <- str_remove(match, \"^Published:\\\\s*\")\n",
        "  match <- str_trim(match)\n",
        "\n",
        "  date_str <- str_extract(match, \"\\\\d{1,2}\\\\s+[A-Za-z]+\\\\s+\\\\d{4}\")\n",
        "  if (is.na(date_str)) return(NA_character_)\n",
        "\n",
        "  parsed_date <- tryCatch(lubridate::dmy(date_str), error = function(e) NA)\n",
        "  if (is.na(parsed_date)) return(NA_character_)\n",
        "\n",
        "  return(format(parsed_date, \"%d/%m/%Y\"))\n",
        "}"
      ],
      "metadata": {
        "id": "RXeqGfov-dRm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Web Scraping Logic\n",
        "get_data <- function(section_url, max_pages, count) {\n",
        "  all_articles <- list()\n",
        "  section_name <- str_extract(section_url, \"(?<=english/)[a-z\\\\-]+\")\n",
        "\n",
        "  for (page_num in (max_pages - count):(max_pages - 1)) {\n",
        "    url <- paste0(section_url, page_num)\n",
        "    # cat(\"ğŸ”„ Reading:\", url, \"\\n\")\n",
        "\n",
        "    page <- tryCatch(read_html(GET(url, ua)), error = function(e) {\n",
        "      message(\"âŒ Failed to load:\", url)\n",
        "      return(NULL)\n",
        "    })\n",
        "\n",
        "    if (is.null(page)) next\n",
        "\n",
        "    # extract elements\n",
        "    title <- tryCatch(page %>% html_element(\"h1\") %>% html_text2(), error = function(e) NA_character_)\n",
        "    ptime_raw <- tryCatch(page %>% html_element(\".Ptime\") %>% html_text2(), error = function(e) NA_character_)\n",
        "    ptime <- clean_published_date(ptime_raw)\n",
        "\n",
        "    content_node <- page %>% html_elements(\"div#content-details\") %>% .[1]\n",
        "    content_text <- tryCatch(html_text2(content_node), error = function(e) NA_character_)\n",
        "\n",
        "    # Check for valid data\n",
        "    if (!is.null(title) && !is.null(content_text) &&\n",
        "          is.character(title) && is.character(content_text) &&\n",
        "          length(title) > 0 && length(content_text) > 0 &&\n",
        "          !is.na(title) && !is.na(content_text) &&\n",
        "          nzchar(title) && nzchar(content_text)) {\n",
        "\n",
        "      cleaned_text <- content_text %>%\n",
        "        str_remove_all(\"googletag\\\\.cmd\\\\.push\\\\(.*?\\\\);\") %>%\n",
        "        str_remove_all(\"\\\\(adsbygoogle = window\\\\.adsbygoogle \\\\|\\\\| \\\\[\\\\]\\\\)\\\\.push\\\\(\\\\{\\\\}\\\\);\") %>%\n",
        "        str_remove_all(\"\\\\}\\\\);\") %>%\n",
        "        str_remove_all(\";\") %>%\n",
        "        str_squish()\n",
        "\n",
        "      all_articles[[length(all_articles) + 1]] <- data.frame(\n",
        "        section = section_name,\n",
        "        url = url,\n",
        "        title = title,\n",
        "        published_date = ptime,\n",
        "        content = cleaned_text,\n",
        "        stringsAsFactors = FALSE\n",
        "      )\n",
        "    } else {\n",
        "      cat(\"âš ï¸ Skipped page due to missing content or title\\n\")\n",
        "    }\n",
        "\n",
        "    # Random delay\n",
        "    delay <- runif(1, min = 0, max = 0)\n",
        "    # cat(\"â³ Sleeping for\", round(delay, 1), \"seconds...\\n\")\n",
        "    Sys.sleep(delay)\n",
        "  }\n",
        "\n",
        "  if (length(all_articles) == 0) return(data.frame())\n",
        "  do.call(rbind, all_articles)\n",
        "}"
      ],
      "metadata": {
        "id": "TatZCN5B-vwj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Link, Max Page & count\n",
        "section_url_science_technology <- c(\"https://www.risingbd.com/english/science-technology/news/\")\n",
        "section_url_politics <- c(\"https://www.risingbd.com/english/politics/news/\")\n",
        "section_url_sports <- c(\"https://www.risingbd.com/english/sports/news/\")\n",
        "section_url_entertainment <- c(\"https://www.risingbd.com/english/entertainment/news/\")\n",
        "section_url_business <- c(\"https://www.risingbd.com/english/business/news/\")\n",
        "section_url_education <- c(\"https://www.risingbd.com/english/education/news/\")\n",
        "section_url_international <- c(\"https://www.risingbd.com/english/international/news/\")\n",
        "section_url_interview <- c(\"https://www.risingbd.com/english/interview/news/\")\n",
        "section_url_country <- c(\"https://www.risingbd.com/english/country/news/\")\n",
        "section_url_national <- c(\"https://www.risingbd.com/english/national/news/\")\n",
        "\n",
        "max_pages_science_technology <- 112396\n",
        "max_pages_politics <- 112225\n",
        "max_pages_sports <- 112423\n",
        "max_pages_entertainment <- 112246\n",
        "max_pages_business <- 112389\n",
        "max_pages_education <- 112354\n",
        "max_pages_international <- 112431\n",
        "max_pages_interview <- 112389\n",
        "max_pages_country <- 112430\n",
        "max_pages_national <- 112436\n",
        "\n",
        "count_1 <- sample(500:1000, 1)\n",
        "count_2 <- sample(500:1000, 1)\n",
        "count_3 <- sample(500:1000, 1)\n",
        "count_4 <- sample(500:1000, 1)\n",
        "count_5 <- sample(500:1000, 1)\n",
        "count_6 <- sample(500:1000, 1)\n",
        "count_7 <- sample(500:1000, 1)\n",
        "count_8 <- sample(500:1000, 1)\n",
        "count_9 <- sample(500:1000, 1)\n",
        "count_10 <- sample(500:1000, 1)\n"
      ],
      "metadata": {
        "id": "0Wh_nIjKBluc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Web Scraping\n",
        "section_data_science_technology <- lapply(section_url_science_technology, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_1, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_science_technology, count = count_1)\n",
        "})\n",
        "\n",
        "section_data_politics <- lapply(section_url_politics, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_2, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_politics, count = count_2)\n",
        "})\n",
        "\n",
        "section_data_sports <- lapply(section_url_sports, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_3, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_sports, count = count_3)\n",
        "})\n",
        "\n",
        "section_data_entertainment <- lapply(section_url_entertainment, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_4, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_entertainment, count = count_4)\n",
        "})\n",
        "\n",
        "section_data_business <- lapply(section_url_business, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_5, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_business, count = count_5)\n",
        "})\n",
        "\n",
        "section_data_education <- lapply(section_url_education, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_5, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_education, count = count_6)\n",
        "})\n",
        "\n",
        "section_data_international <- lapply(section_url_international, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_5, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_international, count = count_7)\n",
        "})\n",
        "\n",
        "section_data_interview <- lapply(section_url_interview, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_5, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_interview, count = count_8)\n",
        "})\n",
        "\n",
        "section_data_country <- lapply(section_url_country, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_5, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_country, count = count_9)\n",
        "})\n",
        "\n",
        "section_data_national <- lapply(section_url_national, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_5, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_national, count = count_10)\n",
        "})"
      ],
      "metadata": {
        "id": "BUg5h4JSgpHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13195cd-254d-45f4-9a55-d2ce74d00fc9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“‚ Scraping section: https://www.risingbd.com/english/science-technology/news/ | ğŸ”¢ Count: 891 \n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "\n",
            "ğŸ“‚ Scraping section: https://www.risingbd.com/english/politics/news/ | ğŸ”¢ Count: 865 \n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "\n",
            "ğŸ“‚ Scraping section: https://www.risingbd.com/english/sports/news/ | ğŸ”¢ Count: 987 \n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "\n",
            "ğŸ“‚ Scraping section: https://www.risingbd.com/english/entertainment/news/ | ğŸ”¢ Count: 728 \n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "\n",
            "ğŸ“‚ Scraping section: https://www.risingbd.com/english/business/news/ | ğŸ”¢ Count: 552 \n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "\n",
            "ğŸ“‚ Scraping section: https://www.risingbd.com/english/education/news/ | ğŸ”¢ Count: 552 \n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "\n",
            "ğŸ“‚ Scraping section: https://www.risingbd.com/english/international/news/ | ğŸ”¢ Count: 552 \n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "\n",
            "ğŸ“‚ Scraping section: https://www.risingbd.com/english/interview/news/ | ğŸ”¢ Count: 552 \n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "\n",
            "ğŸ“‚ Scraping section: https://www.risingbd.com/english/country/news/ | ğŸ”¢ Count: 552 \n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "âš ï¸ Skipped page due to missing content or title\n",
            "\n",
            "ğŸ“‚ Scraping section: https://www.risingbd.com/english/national/news/ | ğŸ”¢ Count: 552 \n",
            "âš ï¸ Skipped page due to missing content or title\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Results\n",
        "df <- bind_rows(\n",
        "  section_data_science_technology,\n",
        "  section_data_politics,\n",
        "  section_data_sports,\n",
        "  section_data_entertainment,\n",
        "  section_data_business,\n",
        "  section_data_education,\n",
        "  section_data_international,\n",
        "  section_data_interview,\n",
        "  section_data_country,\n",
        "  section_data_national)"
      ],
      "metadata": {
        "id": "8nspEq-zJmim"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean <- df[!duplicated(df[, c(\"title\", \"content\")]), ]\n",
        "\n",
        "cat(\"Total Data:\", nrow(df), \"\\n\")\n",
        "cat(\"Duplicates :\", nrow(df) - nrow(df_clean), \"\\n\")\n",
        "cat(\"Available Data:\", nrow(df_clean), \"\\n\")"
      ],
      "metadata": {
        "id": "TzzKMOseqq5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8423905b-272a-4eaa-cf50-135cc39ee207"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Data: 8119 \n",
            "Duplicates : 7045 \n",
            "Available Data: 1074 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert csv & save\n",
        "if (nrow(df_clean) > 0) {\n",
        "  write.csv(df_clean, \"scraped.csv\", row.names = FALSE)\n",
        "  cat(\"âœ… Scraping complete. Data saved to 'scraped.csv'\\n\")\n",
        "} else {\n",
        "  cat(\"âš ï¸ No articles were scraped.\\n\")\n",
        "}"
      ],
      "metadata": {
        "id": "g-DmHlT6rpiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d39650-820e-4837-b83e-fc0204d9558d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Scraping complete. Data saved to 'scraped.csv'\n"
          ]
        }
      ]
    }
  ]
}