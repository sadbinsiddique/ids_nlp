{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyOfxNGHysmINGI1shnGFnQx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadbinsiddique/ids_nlp/blob/main/Data%20Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Web Scraping using R**\n",
        "---\n",
        "**âœ… Scrap URL:** [Rising BD](https://www.risingbd.com/english)\n",
        "\n",
        "**âœ… Install Required Libraries**\n",
        "\n",
        "**âœ… Load Required Libraries**\n",
        "\n",
        "**âœ… Define User-Agent**\n",
        "\n",
        "**âœ… Function to Clean and Format Published Date**\n",
        "\n",
        "**âœ… Function to Web Scraping Logic**\n",
        "*   Extract elements\n",
        "*   Check for valid data\n",
        "*   Random delay\n",
        "\n",
        "**âœ… Load Link & Max Page**\n",
        "\n",
        "**âœ… Start Web Scraping**\n",
        "\n",
        "**âœ… Save Results**\n",
        "\n",
        "**âœ… Convert csv & save**"
      ],
      "metadata": {
        "id": "xihjNeRH4I3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries\n",
        "#install.packages(c(\"rvest\", \"httr\", \"stringr\", \"dplyr\", \"lubridate\"))"
      ],
      "metadata": {
        "id": "RY9mvV0T68b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Required Libraries\n",
        "library(rvest)\n",
        "library(httr)\n",
        "library(stringr)\n",
        "library(dplyr)\n",
        "library(lubridate)"
      ],
      "metadata": {
        "id": "4dEuDOX_4Mwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€ Define User-Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ua <- httr::user_agent(\"Chrome/134.0.0.0\")"
      ],
      "metadata": {
        "id": "_3zmwdsQ6t79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Clean and Format Published Date\n",
        "clean_published_date <- function(raw_text) {\n",
        "  if (is.na(raw_text) || raw_text == \"\") return(NA_character_)\n",
        "\n",
        "  match <- str_extract(raw_text, \"^[^U]+\")\n",
        "  if (is.na(match)) return(NA_character_)\n",
        "\n",
        "  match <- str_remove(match, \"^Published:\\\\s*\")\n",
        "  match <- str_trim(match)\n",
        "\n",
        "  date_str <- str_extract(match, \"\\\\d{1,2}\\\\s+[A-Za-z]+\\\\s+\\\\d{4}\")\n",
        "  if (is.na(date_str)) return(NA_character_)\n",
        "\n",
        "  parsed_date <- tryCatch(lubridate::dmy(date_str), error = function(e) NA)\n",
        "  if (is.na(parsed_date)) return(NA_character_)\n",
        "\n",
        "  return(format(parsed_date, \"%d/%m/%Y\"))\n",
        "}"
      ],
      "metadata": {
        "id": "RXeqGfov-dRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Web Scraping Logic\n",
        "get_data <- function(section_url, max_pages, count) {\n",
        "  all_articles <- list()\n",
        "  section_name <- str_extract(section_url, \"(?<=english/)[a-z\\\\-]+\")\n",
        "\n",
        "  for (page_num in (max_pages - count):(max_pages - 1)) {\n",
        "    url <- paste0(section_url, page_num)\n",
        "    cat(\"ğŸ”„ Reading:\", url, \"\\n\")\n",
        "\n",
        "    page <- tryCatch(read_html(GET(url, ua)), error = function(e) {\n",
        "      message(\"âŒ Failed to load:\", url)\n",
        "      return(NULL)\n",
        "    })\n",
        "\n",
        "    if (is.null(page)) next\n",
        "\n",
        "    # extract elements\n",
        "    title <- tryCatch(page %>% html_element(\"h1\") %>% html_text2(), error = function(e) NA_character_)\n",
        "    ptime_raw <- tryCatch(page %>% html_element(\".Ptime\") %>% html_text2(), error = function(e) NA_character_)\n",
        "    ptime <- clean_published_date(ptime_raw)\n",
        "\n",
        "    content_node <- page %>% html_elements(\"div#content-details\") %>% .[1]\n",
        "    content_text <- tryCatch(html_text2(content_node), error = function(e) NA_character_)\n",
        "\n",
        "    # Check for valid data\n",
        "    if (!is.null(title) && !is.null(content_text) &&\n",
        "          is.character(title) && is.character(content_text) &&\n",
        "          length(title) > 0 && length(content_text) > 0 &&\n",
        "          !is.na(title) && !is.na(content_text) &&\n",
        "          nzchar(title) && nzchar(content_text)) {\n",
        "\n",
        "      cleaned_text <- content_text %>%\n",
        "        str_remove_all(\"googletag\\\\.cmd\\\\.push\\\\(.*?\\\\);\") %>%\n",
        "        str_remove_all(\"\\\\(adsbygoogle = window\\\\.adsbygoogle \\\\|\\\\| \\\\[\\\\]\\\\)\\\\.push\\\\(\\\\{\\\\}\\\\);\") %>%\n",
        "        str_remove_all(\"\\\\}\\\\);\") %>%\n",
        "        str_remove_all(\";\") %>%\n",
        "        str_squish()\n",
        "\n",
        "      all_articles[[length(all_articles) + 1]] <- data.frame(\n",
        "        section = section_name,\n",
        "        url = url,\n",
        "        title = title,\n",
        "        published_date = ptime,\n",
        "        content = cleaned_text,\n",
        "        stringsAsFactors = FALSE\n",
        "      )\n",
        "    } else {\n",
        "      cat(\"âš ï¸ Skipped page due to missing content or title\\n\")\n",
        "    }\n",
        "\n",
        "    # Random delay\n",
        "    delay <- runif(1, min = 0, max = 1)\n",
        "    cat(\"â³ Sleeping for\", round(delay, 1), \"seconds...\\n\")\n",
        "    Sys.sleep(delay)\n",
        "  }\n",
        "\n",
        "  if (length(all_articles) == 0) return(data.frame())\n",
        "  do.call(rbind, all_articles)\n",
        "}"
      ],
      "metadata": {
        "id": "TatZCN5B-vwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Link, Max Page & count\n",
        "section_url_science_technology <- c(\"https://www.risingbd.com/english/science-technology/news/\")\n",
        "section_url_politics <- c(\"https://www.risingbd.com/english/politics/news/\")\n",
        "section_url_sports <- c(\"https://www.risingbd.com/english/sports/news/\")\n",
        "section_url_entertainment <- c(\"https://www.risingbd.com/english/entertainment/news/\")\n",
        "section_url_business <- c(\"https://www.risingbd.com/english/business/news/\")\n",
        "section_url_education <- c(\"https://www.risingbd.com/english/education/news/\")\n",
        "section_url_international <- c(\"https://www.risingbd.com/english/international/news/\")\n",
        "section_url_interview <- c(\"https://www.risingbd.com/english/interview/news/\")\n",
        "section_url_country <- c(\"https://www.risingbd.com/english/country/news/\")\n",
        "section_url_national <- c(\"https://www.risingbd.com/english/national/news/\")\n",
        "\n",
        "max_pages_science_technology <- 112396\n",
        "max_pages_politics <- 112225\n",
        "max_pages_sports <- 112423\n",
        "max_pages_entertainment <- 112246\n",
        "max_pages_business <- 112389\n",
        "max_pages_education <- 112354\n",
        "max_pages_international <- 112431\n",
        "max_pages_interview <- 112389\n",
        "max_pages_country <- 112430\n",
        "max_pages_national <- 112436\n",
        "\n",
        "count_1 <- sample(100:250, 1)\n",
        "count_2 <- sample(100:250, 1)\n",
        "count_3 <- sample(100:250, 1)\n",
        "count_4 <- sample(100:250, 1)\n",
        "count_5 <- sample(100:250, 1)\n",
        "count_6 <- sample(100:250, 1)\n",
        "count_7 <- sample(100:250, 1)\n",
        "count_8 <- sample(100:250, 1)\n",
        "count_9 <- sample(100:250, 1)\n",
        "count_10 <- sample(100:250, 1)\n"
      ],
      "metadata": {
        "id": "0Wh_nIjKBluc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Web Scraping\n",
        "section_data_science_technology <- lapply(section_url_science_technology, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_1, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_science_technology, count = count_1)\n",
        "})"
      ],
      "metadata": {
        "id": "umJ3nWu5cYxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_data_politics <- lapply(section_url_politics, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_2, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_politics, count = count_2)\n",
        "})"
      ],
      "metadata": {
        "id": "PVciJ2OSc27g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_data_sports <- lapply(section_url_sports, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_3, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_sports, count = count_3)\n",
        "})"
      ],
      "metadata": {
        "id": "pUbSGoS_tbTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_data_entertainment <- lapply(section_url_entertainment, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_4, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_entertainment, count = count_4)\n",
        "})"
      ],
      "metadata": {
        "id": "1rqw3pTiySgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_data_business <- lapply(section_url_business, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_5, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_business, count = count_5)\n",
        "})"
      ],
      "metadata": {
        "id": "ZPMFNSIjyU_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_data_education <- lapply(section_url_education, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_6, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_education, count = count_6)\n",
        "})\n"
      ],
      "metadata": {
        "id": "45KHGmL6yW4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_data_international <- lapply(section_url_international, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_7, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_international, count = count_7)\n",
        "})\n"
      ],
      "metadata": {
        "id": "tKinwSW_yYmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_data_interview <- lapply(section_url_interview, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_8, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_interview, count = count_8)\n",
        "})"
      ],
      "metadata": {
        "id": "hOLxH6y8yaTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_data_country <- lapply(section_url_country, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_9, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_country, count = count_9)\n",
        "})"
      ],
      "metadata": {
        "id": "ye0Y6fVfycC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_data_national <- lapply(section_url_national, function(url) {\n",
        "  cat(\"\\nğŸ“‚ Scraping section:\", url, \"| ğŸ”¢ Count:\", count_10, \"\\n\")\n",
        "  get_data(section_url = url, max_pages = max_pages_national, count = count_10)\n",
        "})"
      ],
      "metadata": {
        "id": "Oi5kf7YyydlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Results\n",
        "df <- bind_rows(\n",
        "  section_data_science_technology,\n",
        "  section_data_politics,\n",
        "  section_data_sports,\n",
        "  section_data_entertainment,\n",
        "  section_data_business,\n",
        "  section_data_education,\n",
        "  section_data_international,\n",
        "  section_data_interview,\n",
        "  section_data_country,\n",
        "  section_data_national)"
      ],
      "metadata": {
        "id": "8nspEq-zJmim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert csv & save\n",
        "if (nrow(df) > 0) {\n",
        "  write.csv(df, \"scraped.csv\", row.names = FALSE)\n",
        "  cat(\"âœ… Scraping complete. Data saved to 'scraped.csv'\\n\")\n",
        "} else {\n",
        "  cat(\"âš ï¸ No articles were scraped.\\n\")\n",
        "}"
      ],
      "metadata": {
        "id": "l9WN1mZNJmyR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}